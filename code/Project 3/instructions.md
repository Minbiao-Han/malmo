Note that the value iteration agent we have seen in class does not actually learn from experience. Rather, it ponders its MDP model to arrive at a complete policy before ever interacting with a real environment. When it does interact with the environment, it simply follows the precomputed policy (e.g. it becomes a reflex agent). This distinction may be subtle in a simulated environment, but it's very important in the real world, where the real MDP is not available.

In this project you will implement a Q-learning agent, which does very little on construction, but instead learns by trial and error from interactions with the environment through its update(reward, state, action, nextState) method. A stub of a Q-learner is specified in QLearning.py. For this assignment you will need to complete the updateQTable(), updateQTableFromTerminatingState() and act() methods. 

Complete your Q-learning agent by implementing epsilon-greedy action selection in act(), meaning it chooses random actions an epsilon fraction of the time, and follows its current best Q-values otherwise. Note that choosing a random action may result in choosing the best action - that is, you should not choose a random sub-optimal action, but rather any random legal action.
